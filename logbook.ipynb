{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logbook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNw1kf5mu6GD/+OpMtLG4Tz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnttonLA/BINP37/blob/master/logbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtYoNzSZWARW",
        "colab_type": "text"
      },
      "source": [
        "# 31 - 03 - 2020 (And before)\n",
        "\n",
        "**Aim:** Set up bioBERT in Colab. Make it work.\n",
        "\n",
        "**Result:** After some compatibility issues, bioBERT seems to be working. Fine-tunning was performed.\n",
        "\n",
        "**Next step:** Use bioBERT to make predictions with the fine-tuned model.\n",
        "\n",
        "I found a really useful comment on the tensorflow GitHub about installing an older version of Cuda, which is necessary for BioBERT to run: https://github.com/tensorflow/tensorflow/issues/15604#issuecomment-368286864"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glaNtIkpIFug",
        "colab_type": "text"
      },
      "source": [
        "# 02 - 04 - 2020\n",
        "\n",
        "**Aim:** Confirm that BioBERT performs prediction not only for the given datasets, but for custom datasets as well.\n",
        "\n",
        "**Result:** After a lot of struggling, we see that it can indeed use our data, but it needs some workarounds and the data needs to be *very* specifically formated.\n",
        "\n",
        "**Next step:** Format our data in the desired way so that we can perfrom prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhHWMTS0ZQvv",
        "colab_type": "text"
      },
      "source": [
        "#05 - 04 - 2020\n",
        "**Aim:** Get a 'test.tsv' file that is compatible with bioBERTs prediciton.\n",
        "\n",
        "**Result:** Managed to figure out the exact format of the dataset required for bioBERT to work properly. I wrote some preliminary scripts that make text into that format. I was able to get a rough version of the full pipeline, obtaining the Named Entities of the text.\n",
        "\n",
        "**Next step:** Perfect the pipeline. Specifically, improve on the scripts that format the data in the correct way to be used in BioBERT.\n",
        "\n",
        "On paralell with this, I completed a \"guide\" on how to set up BioBERT in Colab, as I thought it'd be usefull to others (and me) in the future. It can be found here: https://github.com/AnttonLA/BINP37/blob/master/bioBERT.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaOs5oadoqMw",
        "colab_type": "text"
      },
      "source": [
        "#07 - 04 - 2020\n",
        "\n",
        "**Aim:** Transform the whole 100 paper subset to the appropriate tsv format in order for it to work in BioBERT\n",
        "\n",
        "**Result:** I fixed the script that creates the tsv. Right now it creates a working file from a single paper, and the prediction works fine in that paper.\n",
        "\n",
        "**Next Step:** Generalize the script so that it works in all 100 papers. Get the output file and transform the output to PubAnnotation format.\n",
        "\n",
        "I decided to use some extra time to watch some YouTube videos and do some reading (re-read the main paper and part of the BERN paper). It was very much worth it: I learned a lot in the last couple of days about BERT, BioBERT and NLP in general and that helped with the coding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fm21g0i6JAR",
        "colab_type": "text"
      },
      "source": [
        "# 08 - 04 - 2020\n",
        "\n",
        "**Aim:** Create custom script that will generate a 'test.tsv' file of the appropriate format from the 100 paper subset, so that it can be used in BioBERT. Do prediction in BioBERT with it.\n",
        "\n",
        "**Result:** I finished the pipeline to generate '.tsv' files to use for prediction from the JSON files. The scripts are here: https://github.com/AnttonLA/BINP37/tree/master/dataset_generation\n",
        "Prediction with the 100 papers accomplished. Started working on a script that will transform the output file to PubAnnotation format, which proved trickier than expected.\n",
        "\n",
        "**Next Step:** Finish that last script, upload the next JSON file.\n",
        "\n",
        "The methods used by the BioBERT team for data pre-processing were not made public. I managed to approximate the pre-processing taking inspiration from https://github.com/spyysalo/standoff2conll/blob/master/common.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6jMNgpGpJXx",
        "colab_type": "text"
      },
      "source": [
        "# 13 - 04 - 2020\n",
        "\n",
        "**Aim:** Reproduce results from the BioBERT paper for three of the \"default\" datasets.\n",
        "\n",
        "**Result:** I got Precision, Recall and F-Score for three models: *NCBI-Disease*, *BC5CDR-Chemicals* and *JNLPBA*. The results are close to those shown in the BioBERT paper. I stored the fine-tuned models in my Drive since the process is quite time consuming. This way I can just load them from there instead of doing it all over again.\n",
        "\n",
        "**Next Step:** Finish the upload to PubAnnotation.\n",
        "\n",
        "Also in the #TODO list: update/slightly improve on the BioBERT guide, and improve on the documentation in several files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVkZGzq2U6uM",
        "colab_type": "text"
      },
      "source": [
        "#15 - 04 - 2020\n",
        "\n",
        "**Aim:** Finish the script that converts the output to PubAnnotation\n",
        "\n",
        "**Result:** It could be more solid, but the the script it done and seems to work fine.\n",
        "\n",
        "**Next Step:** Obtain final PubAnnotation files of the 100 paper subset\n",
        "\n",
        "Also, improve on guide and documentation, as well as project architecture. It's time for a little file cleanup! \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWx5ALXCU6xo",
        "colab_type": "text"
      },
      "source": [
        "# 16 - 04 - 2020\n",
        "\n",
        "**Aim:** Generate the final JSON files that correspond to the 100 paper subset.\n",
        "\n",
        "**Result:** Once again, a slight rework of the formatting scripts was required, but it worked out in the end. I created all the JSON files.\n",
        "The final scripts (as well as the compressed files) can be found here:\n",
        "\n",
        " https://github.com/AnttonLA/BINP37/tree/master/dataset_generation\n",
        "\n",
        " https://github.com/AnttonLA/BINP37/tree/master/output_generation\n",
        "\n",
        "\n",
        "**Next Step:** Cleanup + start looking into setting BioBERT up in Lunarc so that I can train a proper model to check the gold standard dataset with. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAr2qUSMvYH6",
        "colab_type": "text"
      },
      "source": [
        "#17 - 04 - 2020\n",
        "\n",
        "**Aim:** Learn how to access and use Lunarc. \n",
        "\n",
        "BONUS: figure out why the PubAnnotation format JSON files have the wrong text assigned.\n",
        "\n",
        "**Results:** I read the guides provided by SNIC. However a lot of things are still unclear to me.\n",
        "\n",
        "I discovered that when extracting the text from the files of the 100 paper dataset I was skipping over parts of the abstract in the cases where the abstract was composed by multiple paragraphs. Not sure if this was the cause of the miss-alignment of text/name in the final output.\n",
        "\n",
        "**Next Step:** Top priority is to keep investigating the miss-alingment and fix it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-FCNuVlvYby",
        "colab_type": "text"
      },
      "source": [
        "#20 - 04 - 2020\n",
        "\n",
        "**Aim:** Fix the output files.\n",
        "\n",
        "**Result:** Issue fixed. It was caused by the fact that a few files from the database lacked titles or had multi-paragraph abstracts. \n",
        "\n",
        "**Next Step:** Back to learning about Lunarc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID9HFDriZPOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_GdiVhArbDC",
        "colab_type": "text"
      },
      "source": [
        "#22 - 04 - 2020\n",
        "\n",
        "**Aim:** The failes turned to be faulty again. The files have a suspiciously low amount of entities in them. This required some exploring.\n",
        "\n",
        "**Result:** The lack of Entitities seems to be partially caused by the large length of the paragraphs. If the paragraphs are too long they get trimmed by BioBERT and only the un-trimmed part is analyzed. I increased the \"sentence length\" to the allowed maximum, which still leaves chunks of text out, but should be much better.\n",
        "\n",
        "**Next Step:** I still want to investigate this issue a little more. In particular, the results of the default-datasets seem to have been altered by a file that was newly updated in BioBERT, and this obviously shouldn't be the case.\n",
        "\n",
        "\n",
        "Also, I had a brief meeting with Salma on the 21st, after the main meeting. She gave me a general overcview about how to use Lunarc and answered all of my questions. It was VERY helpful!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVxJVNL4xQB-",
        "colab_type": "text"
      },
      "source": [
        "# 25 - 04 - 2020\n",
        "\n",
        "**Aim:** Investigate detokenization/sentence length more in depth for possble errors. Lunarc setup.\n",
        "\n",
        "**Result:** I THINK it works properly now. It still ignores sentences that are too long. It would be possible to build a workaround if necessary I think.\n",
        "\n",
        "**Next Step:** Keep figuring out how to make BioBERT work in Lunarc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm1qFd0RxBjY",
        "colab_type": "text"
      },
      "source": [
        "# 27 - 04 - 2020\n",
        "\n",
        "**Aim:** After the meeting today I was asked to perform some tests to see if BioBERT was working correctly. I wanted to try to set up BioBERT in Lunarc first, however, as I suspect some of my results have been inconsistent because of how unreliable colab is.\n",
        "\n",
        "**Results:** I made some progress, but I haven't figured out how to make it work yet. I will ask for help with this and do the tests in Colab.\n",
        "\n",
        "**Next Step:** Make the required tests in BioBERT. Send the result table to Sonja!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw4Q271_5WEX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wtCOUAb5WMA",
        "colab_type": "text"
      },
      "source": [
        "# 29 - 04 - 2020\n",
        "\n",
        "**Aim:** Perform the evaluation and put the results up on the labs GitHub. Also update the BioBERT guide, explaining how to use the text-formatting scripts needed to ready the data for BioBERT.\n",
        "\n",
        "**Result:** I updated the guide to include an explanation for my formatting scripts so that other people may use them more easily if they want to.\n",
        "\n",
        "I performed the evaluation for three of the models, each with its corresponding default datasets. I had done this before but this time I used max sentence length which imporved results. I saved all necesary files and data, and composed a table with the results. \n",
        "\n",
        "I started writing a script that will extract the information of the gold-standard JSON file. The idea is that it will create a proper BioBERT test file with the correct tags, which will allow me to use the native BioBERT evaluation scripts. However as alsways formatting for BioBERT is a little tricky.\n",
        "\n",
        "**Next Step:** Finish up the 'gold_to_tsv.py' script. Perform the evaluation with the resulting file. Update all to both my personal GitHub and the labs GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgWXxsvKXYSC",
        "colab_type": "text"
      },
      "source": [
        "# 04 - 05 - 2020\n",
        "\n",
        "**Aim:** Finish the 'gold_to_tsv.py' script so that it produces both an output file and a reference file that will allow converting the BioBERT output to PubAnnotation format.\n",
        "\n",
        "**Result:** The script is finished! (I'll put the link to it here as soon as I put it up in my GitHub page). I have results for the gold-standards dataset.\n",
        "\n",
        "**Next Step:** Upload all the evaluation data gathered in the past week to the BioNLP GitHub page! Start writing the final report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfuSD2OlXYhb",
        "colab_type": "text"
      },
      "source": [
        "# 07 - 05 - 2020\n",
        "\n",
        "**Aim:** Generalize the script used to make the gold standard into a tsv so it will work afterwards to make any json of Pubannotation format into a tsv test set for BioBERT.\n",
        "\n",
        "Also, get results for the 100 paper subset with two models (NCBI-Disease and JNLPBA) so that the Relationship Extraction group can use them.\n",
        "\n",
        "**Result:** Results obtained and uploaded, althought they are not perfect. Script is not yet finished entirely.\n",
        "\n",
        "**Next Step:** The produced output files have 3 small mistakes (that I know of):\n",
        "\n",
        "1.   The divid index is wrong (starts at 1, should start at 0). \n",
        "2.   The text has extra spaces that shouldn't be there (even though the \"spans\" of the denotations take them into account so they are correct).\n",
        "3. A few of the paragraphs are too long for BioBERT to handle, so the entities at the end of these paragraphs have been skipped.\n",
        "\n",
        "I want to fix mistakes 1 and 2 at least. 3 would be considerably trickier to solve (although doable) and I don't consider it to be a priority right now.\n",
        "\n",
        "Continuing to write the report is also in the back of my mind.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_sXIv97vqKo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTXWWs2wvqVm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}