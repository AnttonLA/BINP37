{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logbook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMk2ETb2PNbFQmnnl+Hdfrk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnttonLA/BINP37/blob/master/logbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtYoNzSZWARW",
        "colab_type": "text"
      },
      "source": [
        "# 31 - 03 - 2020 (And before)\n",
        "\n",
        "**Aim:** Set up bioBERT in Colab. Make it work.\n",
        "\n",
        "**Result:** After some compatibility issues, bioBERT seems to be working. Fine-tunning was performed.\n",
        "\n",
        "**Next step:** Use bioBERT to make predictions with the fine-tuned model.\n",
        "\n",
        "I found a really useful comment on the tensorflow GitHub about installing an older version of Cuda, which is necessary for BioBERT to run: https://github.com/tensorflow/tensorflow/issues/15604#issuecomment-368286864"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glaNtIkpIFug",
        "colab_type": "text"
      },
      "source": [
        "# 02 - 04 - 2020\n",
        "\n",
        "**Aim:** Confirm that BioBERT performs prediction not only for the given datasets, but for custom datasets as well.\n",
        "\n",
        "**Result:** After a lot of struggling, we see that it can indeed use our data, but it needs some workarounds and the data needs to be *very* specifically formated.\n",
        "\n",
        "**Next step:** Format our data in the desired way so that we can perfrom prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhHWMTS0ZQvv",
        "colab_type": "text"
      },
      "source": [
        "#05 - 04 - 2020\n",
        "**Aim:** Get a 'test.tsv' file that is compatible with bioBERTs prediciton.\n",
        "\n",
        "**Result:** Managed to figure out the exact format of the dataset required for bioBERT to work properly. I wrote some preliminary scripts that make text into that format. I was able to get a rough version of the full pipeline, obtaining the Named Entities of the text.\n",
        "\n",
        "**Next step:** Perfect the pipeline. Specifically, improve on the scripts that format the data in the correct way to be used in BioBERT.\n",
        "\n",
        "On paralell with this, I completed a \"guide\" on how to set up BioBERT in Colab, as I thought it'd be usefull to others (and me) in the future. It can be found here: https://github.com/AnttonLA/BINP37/blob/master/bioBERT.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaOs5oadoqMw",
        "colab_type": "text"
      },
      "source": [
        "#07 - 04 - 2020\n",
        "\n",
        "**Aim:** Transform the whole 100 paper subset to the appropriate tsv format in order for it to work in BioBERT\n",
        "\n",
        "**Result:** I fixed the script that creates the tsv. Right now it creates a working file from a single paper, and the prediction works fine in that paper.\n",
        "\n",
        "**Next Step:** Generalize the script so that it works in all 100 papers. Get the output file and transform the output to PubAnnotation format.\n",
        "\n",
        "I decided to use some extra time to watch some YouTube videos and do some reading (re-read the main paper and part of the BERN paper). It was very much worth it: I learned a lot in the last couple of days about BERT, BioBERT and NLP in general and that helped with the coding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fm21g0i6JAR",
        "colab_type": "text"
      },
      "source": [
        "# 08 - 04 - 2020\n",
        "\n",
        "**Aim:** Create custom script that will generate a 'test.tsv' file of the appropriate format from the 100 paper subset, so that it can be used in BioBERT. Do prediction in BioBERT with it.\n",
        "\n",
        "**Result:** I finished the pipeline to generate '.tsv' files to use for prediction from the JSON files. The scripts are here: https://github.com/AnttonLA/BINP37/tree/master/dataset_generation\n",
        "Prediction with the 100 papers accomplished. Started working on a script that will transform the output file to PubAnnotation format, which proved trickier than expected.\n",
        "\n",
        "**Next Step:** Finish that last script, upload the next JSON file.\n",
        "\n",
        "The methods used by the BioBERT team for data pre-processing were not made public. I managed to approximate the pre-processing taking inspiration from https://github.com/spyysalo/standoff2conll/blob/master/common.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6jMNgpGpJXx",
        "colab_type": "text"
      },
      "source": [
        "# 13 - 04 - 2020\n",
        "\n",
        "**Aim:** Reproduce results from the BioBERT paper for three of the \"default\" datasets.\n",
        "\n",
        "**Result:** I got Precision, Recall and F-Score for three models: *NCBI-Disease*, *BC5CDR-Chemicals* and *JNLPBA*. The results are close to those shown in the BioBERT paper. I stored the fine-tuned models in my Drive since the process is quite time consuming. This way I can just load them from there instead of doing it all over again.\n",
        "\n",
        "**Next Step:** Finish the upload to PubAnnotation.\n",
        "\n",
        "Also in the #TODO list: update/slightly improve on the BioBERT guide, and improve on the documentation in several files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVkZGzq2U6uM",
        "colab_type": "text"
      },
      "source": [
        "#15 - 04 - 2020\n",
        "\n",
        "**Aim:** Finish the script that converts the output to PubAnnotation\n",
        "\n",
        "**Result:** It could be more solid, but the the script it done and seems to work fine.\n",
        "\n",
        "**Next Step:** Obtain final PubAnnotation files of the 100 paper subset\n",
        "\n",
        "Also, improve on guide and documentation, as well as project architecture. It's time for a little file cleanup! \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWx5ALXCU6xo",
        "colab_type": "text"
      },
      "source": [
        "# 16 - 04 - 2020\n",
        "\n",
        "**Aim:** Generate the final JSON files that correspond to the 100 paper subset.\n",
        "\n",
        "**Result:** Once again, a slight rework of the formatting scripts was required, but it worked out in the end. I created all the JSON files.\n",
        "The final scripts (as well as the compressed files) can be found here:\n",
        "\n",
        " https://github.com/AnttonLA/BINP37/tree/master/dataset_generation\n",
        "\n",
        " https://github.com/AnttonLA/BINP37/tree/master/output_generation\n",
        "\n",
        "\n",
        "**Next Step:** Cleanup + start looking into setting BioBERT up in Lunarc so that I can train a proper model to check the gold standard dataset with. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAr2qUSMvYH6",
        "colab_type": "text"
      },
      "source": [
        "#17 - 04 - 2020\n",
        "\n",
        "**Aim:** Learn how to access and use Lunarc. \n",
        "\n",
        "BONUS: figure out why the PubAnnotation format JSON files have the wrong text assigned.\n",
        "\n",
        "**Results:** I read the guides provided by SNIC. However a lot of things are still unclear to me.\n",
        "\n",
        "I discovered that when extracting the text from the files of the 100 paper dataset I was skipping over parts of the abstract in the cases where the abstract was composed by multiple paragraphs. Not sure if this was the cause of the miss-alignment of text/name in the final output.\n",
        "\n",
        "**Next Step:** Top priority is to keep investigating the miss-alingment and fix it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-FCNuVlvYby",
        "colab_type": "text"
      },
      "source": [
        "#20 - 04 - 2020\n",
        "\n",
        "**Aim:** Fix the output files.\n",
        "\n",
        "**Result:** Issue fixed. It was caused by the fact that a few files from the database lacked titles or had multi-paragraph abstracts. \n",
        "\n",
        "**Next Step:** Back to learning about Lunarc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_GdiVhArbDC",
        "colab_type": "text"
      },
      "source": [
        "#22 - 04 - 2020\n",
        "\n",
        "**Aim:** The failes turned to be faulty again. The files have a suspiciously low amount of entities in them. This required some exploring.\n",
        "\n",
        "**Result:** The lack of Entitities seems to be partially caused by the large length of the paragraphs. If the paragraphs are too long they get trimmed by BioBERT and only the un-trimmed part is analyzed. I increased the \"sentence length\" to the allowed maximum, which still leaves chunks of text out, but should be much better.\n",
        "\n",
        "**Next Step:** I still want to investigate this issue a little more. In particular, the results of the default-datasets seem to have been altered by a file that was newly updated in BioBERT, and this obviously shouldn't be the case.\n",
        "\n",
        "\n",
        "Also, I had a brief meeting with Salma on the 21st, after the main meeting. She gave me a general overcview about how to use Lunarc and answered all of my questions. It was VERY helpful!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVxJVNL4xQB-",
        "colab_type": "text"
      },
      "source": [
        "# 25 - 04 - 2020\n",
        "\n",
        "**Aim:** Investigate detokenization/sentence length more in depth for possble errors. Lunarc setup.\n",
        "\n",
        "**Result:** I THINK it works properly now. It still ignores sentences that are too long. It would be possible to build a workaround if necessary I think.\n",
        "\n",
        "**Next Step:** Keep figuring out how to make BioBERT work in Lunarc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm1qFd0RxBjY",
        "colab_type": "text"
      },
      "source": [
        "# 27 - 04 - 2020\n",
        "\n",
        "**Aim:** After the meeting today I was asked to perform some tests to see if BioBERT was working correctly. I wanted to try to set up BioBERT in Lunarc first, however, as I suspect some of my results have been inconsistent because of how unreliable colab is.\n",
        "\n",
        "**Results:** I made some progress, but I haven't figured out how to make it work yet. I will ask for help with this and do the tests in Colab.\n",
        "\n",
        "**Next Step:** Make the required tests in BioBERT. Send the result table to Sonja!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wtCOUAb5WMA",
        "colab_type": "text"
      },
      "source": [
        "# 29 - 04 - 2020\n",
        "\n",
        "**Aim:** Perform the evaluation and put the results up on the labs GitHub. Also update the BioBERT guide, explaining how to use the text-formatting scripts needed to ready the data for BioBERT.\n",
        "\n",
        "**Result:** I updated the guide to include an explanation for my formatting scripts so that other people may use them more easily if they want to.\n",
        "\n",
        "I performed the evaluation for three of the models, each with its corresponding default datasets. I had done this before but this time I used max sentence length which imporved results. I saved all necesary files and data, and composed a table with the results. \n",
        "\n",
        "I started writing a script that will extract the information of the gold-standard JSON file. The idea is that it will create a proper BioBERT test file with the correct tags, which will allow me to use the native BioBERT evaluation scripts. However as alsways formatting for BioBERT is a little tricky.\n",
        "\n",
        "**Next Step:** Finish up the 'gold_to_tsv.py' script. Perform the evaluation with the resulting file. Update all to both my personal GitHub and the labs GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgWXxsvKXYSC",
        "colab_type": "text"
      },
      "source": [
        "# 04 - 05 - 2020\n",
        "\n",
        "**Aim:** Finish the 'gold_to_tsv.py' script so that it produces both an output file and a reference file that will allow converting the BioBERT output to PubAnnotation format.\n",
        "\n",
        "**Result:** The script is finished! (I'll put the link to it here as soon as I put it up in my GitHub page). I have results for the gold-standards dataset.\n",
        "\n",
        "**Next Step:** Upload all the evaluation data gathered in the past week to the BioNLP GitHub page! Start writing the final report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfuSD2OlXYhb",
        "colab_type": "text"
      },
      "source": [
        "# 07 - 05 - 2020\n",
        "\n",
        "**Aim:** Generalize the script used to make the gold standard into a tsv so it will work afterwards to make any json of Pubannotation format into a tsv test set for BioBERT.\n",
        "\n",
        "Also, get results for the 100 paper subset with two models (NCBI-Disease and JNLPBA) so that the Relationship Extraction group can use them.\n",
        "\n",
        "**Result:** Results obtained and uploaded, althought they are not perfect. Script is not yet finished entirely.\n",
        "\n",
        "**Next Step:** The produced output files have 3 small mistakes (that I know of):\n",
        "\n",
        "1.   The divid index is wrong (starts at 1, should start at 0). \n",
        "2.   The text has extra spaces that shouldn't be there (even though the \"spans\" of the denotations take them into account so they are correct).\n",
        "3. A few of the paragraphs are too long for BioBERT to handle, so the entities at the end of these paragraphs have been skipped.\n",
        "\n",
        "I want to fix mistakes 1 and 2 at least. 3 would be considerably trickier to solve (although doable) and I don't consider it to be a priority right now.\n",
        "\n",
        "Continuing to write the report is also in the back of my mind.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_sXIv97vqKo",
        "colab_type": "text"
      },
      "source": [
        "# 08 - 05 - 2020\n",
        "\n",
        "**Aim:** Fix the issue with the extra spaces in the text. I contacted Marcus and he suggested an algorithm that looks very promising, I'll try implementing it!\n",
        "\n",
        "**Result:** Implementation is almost done, just needs some finishing touches.\n",
        "\n",
        "**Next Step:** Finish up this script and solve issue #1 (the divid index) as well, then upload the results for the Relationship Extraction group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTXWWs2wvqVm",
        "colab_type": "text"
      },
      "source": [
        "# 11 - 05 - 2020\n",
        "\n",
        "**Aim:** Produce and upload corrected PubAnnotation files. Look into generating a dataset for fine-tunning, since that's what I'll need to do with the outputs of the dictionary group.\n",
        "\n",
        "**Result:** The re-worked script that produces PubAnnotation files from BioBERT outputs can be found here: https://github.com/AnttonLA/BINP37/blob/master/output_generation/eval_to_pubannot.py\n",
        "\n",
        "It should be much more robust than before. My thanks to Marcus Klang for his advice!\n",
        "\n",
        "Output was produced using the NCBI_disease and JNLPBA models for the 100 paper subset, as requested by the Relationship Extraction group.\n",
        "\n",
        "**Next Step:** The evaluation of the Gold Standard should be re-done, since the results seemed inconsistent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WPo1WzW465o",
        "colab_type": "text"
      },
      "source": [
        "# 14 - 05 - 2020\n",
        "\n",
        "**Aim:** A bug was detected in the *eval_to_pubanot.py* script. Needs fixing! Apart from that, continue working on the *pubanot_to_tsv.py*. The aim of this script is to take data from PubAnnotation files (the gold standard or the results of other teams) and re-format it to a BioBERT compatible file. \n",
        "\n",
        "**Results:** Bug fixed! The script should work as intended now. Other than that, I made good progress in writing *pubannot_to_tsv.py*!\n",
        "\n",
        "**Next Step:** Once *pubannot_to_tsv.py* is working I will use it to re-do the evaluation of the Gold Standard!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUIDPk7XGY0d",
        "colab_type": "text"
      },
      "source": [
        "#17 - 05 - 2020\n",
        "\n",
        "**Aim:** Finish *pubannot_to_tsv.py* and evaluate the gold standard.\n",
        "\n",
        "**Results:** After much de-bugging *pubannot_to_tsv.py* is finished! The script can be found here: https://github.com/AnttonLA/BINP37/tree/master/dataset_from_pubannot\n",
        "\n",
        "The new script is able to discriminate denotations depending on the tags, so that only the desired denotations are included in the analysis. \n",
        "\n",
        "I re-run the analysis of the gold standard dataset (only the denotations taged as 'protein') using the JNLPBA model. When evaluating the results, I noticed some issues with the native BioBERT evaluation code. It seems a few of the tags set by me in the 'test.tsv' file get moved around for some reason. \n",
        "\n",
        "**Next Step:** Try to understand what BioBERT is doing here. Otherwise, just generate the PubAnnotation file from the output and use the evaluator Annie and Sofi developed instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW2qJbuAGcza",
        "colab_type": "text"
      },
      "source": [
        "**Aim:** Get the prediction of the NCBI_disease model for the gold standard. Run the evaluation for both of the re-run gold standard outputs.\n",
        "\n",
        "**Results:** I didn't manage to make the BioBERT evaluation work properly. I then tried to use Annie and Sofi's but I couldn't make it work either...\n",
        "\n",
        "**Next Step:** Figure out what to do with the evaluation. Otherwise start working on expanding the training set!\n",
        "\n",
        "I also recieved Williams results which I'm supposed to add to my own!\n",
        "\n",
        "And writting is a little behind too! :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8FqLgWQ2Joo",
        "colab_type": "text"
      },
      "source": [
        "# 20 - 05 - 2020\n",
        "\n",
        "**Aim:** Perform the cross-check between BioBERT models and their datasets.\n",
        "\n",
        "**Results:** I checked the BC4CHEMD corpus agains the BC5CDR Chem model, the BC5CDR-disease corpus with the NCBI-disease model, and the BC2GM corpus with the JNLPBA model. The results can be found here, in the evaluation table: https://github.com/Aitslab/BioNLP/tree/master/evaluation/antton\n",
        "\n",
        "I also tweaked the gold-standard corpus so that the filenames match with the one produced by my script *(\" 'cord_uid' - 'divid' - [section of text] . json \")*, and fused 'Disease_COVID-19' and 'Disease_other' into just 'Disease'. These two things are necessary for the evaluator.\n",
        "\n",
        "**Next Step:** Keep working on evaluating the results obtained for the gold standard!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzTC8DLYrujl",
        "colab_type": "text"
      },
      "source": [
        "# 21 - 05 - 2020\n",
        "\n",
        "**Aim:** I haven't been able to solve the issues with either the BioBERT evaluation or Annie and Sofi's evaluatior. Since the rest of the groups will be using it too, I'll focus on getting Annie and Sofi's evaluator to work.\n",
        "\n",
        "**Result:** It took waaaay longer than expected, but I managed to make the evaluator work by tweaking the existing code a little. My results have been uploaded to: https://github.com/Aitslab/BioNLP/tree/master/evaluation/antton\n",
        "\n",
        "**Next Step:** Look into expanding the training set for fine-tunning with BioBERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP2us1uA4-7u",
        "colab_type": "text"
      },
      "source": [
        "# 23 - 05 - 2020\n",
        "\n",
        "**Aim:** Make a training dataset out of Williams results for the 100 paper subset.\n",
        "\n",
        "**Result:** I built two different datasets using Willams data. One that contains exclusively Willaims results, and another one that is the result of joining the NCBI-disease database with William's data. I fine-tuned bioBERT with both of those datasets and then performed a prediction for the gold-standard dataset using those results.\n",
        "\n",
        "**Next Step:** There's still a lot to be done regarding the models, but I want to focus on writing the report for a short while in order to have a draft to show to Sonja before the summer vacation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6DXFabv3OaC",
        "colab_type": "text"
      },
      "source": [
        "# 25 - 05 - 2020\n",
        "**Aim:** Advance the writing of the report.\n",
        "\n",
        "**Result:** I finished the abstract and a good chunk of the introduction!\n",
        "\n",
        "**Next Step:** Keep on writing! My intention is to have a preliminary draft finished by the end of this week."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJXL46nQEPYM",
        "colab_type": "text"
      },
      "source": [
        "# 27 - 05 - 2020\n",
        "\n",
        "**Aim:** Advance the writing. Check the False Positives and False Negatives obtained in the evaluation for the model fine-tuned with the NCBI-disease. The results are not very good at the moment and it would be good to understand *why*.\n",
        "\n",
        "**Result:** I've fixed a big that made a few of the spans of the BioBERT results innacurate.\n",
        "\n",
        "It looks like pretty much all of the False Positives are coming from BioBERT correctly detecting terms that are tagged differently on the gold standard. An example: I switched the tag *Symptom* to *Disease* in the Gold Standard, and the precision went from 33% to 52% because BioBERT had selected every single one of them.\n",
        "\n",
        "In regards to the writting, I'm done with the introduction and have started working on the methodology.\n",
        "\n",
        "**Next Step:** Keep on with the writting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM8K-TxTiyVk",
        "colab_type": "text"
      },
      "source": [
        "# 29 - 05 - 2020\n",
        "\n",
        "**Aim:** Advance with the writting\n",
        "\n",
        "**Results:** The first draft of the report is almost finished. I still need to write out some of the sections and I'm of course missing some results, but the overall structure has been defined.\n",
        "\n",
        "**Next Step:** Finishing touches to the first draft. Also, look into potential databases that could be used to fine-tune models specific to the symptoms tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSPFWagugZMu",
        "colab_type": "text"
      },
      "source": [
        "#1 - 06 - 2020\n",
        "\n",
        "**Aim:** Look into the Mondo ontology to extract disease names for potential dictionary-tagger use. Writing.\n",
        "\n",
        "**Results:** I finished the results section with the results I have so far (I'm planning on hopefully expanding on them greatly) and gave some other finishing touches.\n",
        "\n",
        "I completed a short script that takes disease-names form the Mondo JSON, but later realised Sonja specifically asked to parse the OBO or OWL format files :(\n",
        "\n",
        "**Next Step:** Continue working on improving the models.\n",
        "\n",
        "I haven't advanced nearly as much as I should have in the last couple of weeks, and while the delayed submission date gives me some extra time I'd like to avoid working too much on this during the summer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWwO1V2OlBoP",
        "colab_type": "text"
      },
      "source": [
        "##2 - 06 - 2020\n",
        "\n",
        "**Aim:** Fine-tune new models with protein data from William's results.\n",
        "\n",
        "**Results:** I have two new models, one trained purely with William's results and one trained with the combination of JNLPBA and the results. Both models are **HORRIBLE**, at least with the Gold Standard.\n",
        "\n",
        "**Next Step:** Ask for guidance. Having access to a Silver Standard corpus would make the evaluation more trustworthy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtyZwdZqeO6e",
        "colab_type": "text"
      },
      "source": [
        "# 3 - 06 - 2020\n",
        "\n",
        "**Aim:** (The protein model doesn't seem to work at all, so I'll focus on the disease model until I get more data.) Test which disease model is the best one. Tweak the fine-tuning done with William's data to see if the stats vary.\n",
        "\n",
        "**Results:** I run the eval of disease models again, this time using BioBERT evaluation file. I don't know if it's 100% trustworthy but it's much much easier to run repeatedly and the results seem consistent. I tried the fine-tuning with different learning rates and number of epochs, but the default values seem to work best so far.\n",
        "\n",
        "**Next Step:** Continue tweaking parameters. I might be limited in data but I can at least investigate which fine-tuning parameters work best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHdGjQuVmGhA",
        "colab_type": "text"
      },
      "source": [
        "8 - 06 - 2020\n",
        "\n",
        "**Aim:** Look into improving the Mondo disease name list. Fine-tune a new model from the Annie and Sofi symptom data.\n",
        "\n",
        "**Results:** Removed duplicates and corrected names in the Mondo file. List went down from 127.225 names to 124.412 names. Symptoms model was fine-tuned, although results are not grat.\n",
        "\n",
        "**Next Step:** Updating my scripts in GitHub and the results table. Should have done it ages ago. Test the model fine-tuned with Williams data with the BioBERT default disease test files. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKC1YkAGtdtK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAF_wdSZtd4j",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeQ48HbDteBr",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}